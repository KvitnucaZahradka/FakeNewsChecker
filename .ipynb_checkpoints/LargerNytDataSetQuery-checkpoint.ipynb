{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import calendar\n",
    "import logging\n",
    "\n",
    "import urllib\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "import requests\n",
    "import html.parser\n",
    "from requests.exceptions import HTTPError\n",
    "from socket import error as SocketError\n",
    "from http.cookiejar import CookieJar\n",
    "\n",
    "import feedparser\n",
    "import pickle\n",
    "\n",
    "class NytData:\n",
    "    # FIELDS\n",
    "    __offset = 'offset=0'\n",
    "    # you use just 9000 out of 10000 to have some buffer\n",
    "    __totalNumberOfAllowedQueries = 9000\n",
    "    __numberOfUsedQueries = 0\n",
    "    __nytApiKey = ''\n",
    "    __queryList = []\n",
    "    __paperName = ''\n",
    "\n",
    "    # CLASS INITIALIZATION\n",
    "    def __init__(self, apiKey, paperName, totalNumberOfAllowedQueries=9000):\n",
    "        try:\n",
    "            ##  NOTE: add conditions to if statement\n",
    "            if((type(apiKey) and type(paperName)) == str):\n",
    "                self.__nytApiKey = 'api-key=' + apiKey\n",
    "                self.__paperName = paperName\n",
    "\n",
    "                # add the required query list by hand\n",
    "                self.__queryList = [\"Politics\", \"Economics\", \"Business\", \"Financial\", \"World\",\n",
    "                                              \"Washington\", \"Wealth\", \"Jobs\", \"Society\"]\n",
    "                # self.__queryList = queryList\n",
    "                self.__totalNumberOfAllowedQueries = totalNumberOfAllowedQueries\n",
    "            else:\n",
    "                raise ValueError\n",
    "        except ValueError as e:\n",
    "            print('there has been error values in initialization')\n",
    "\n",
    "\n",
    "\n",
    "    # HELPFUL FUNCTIONS (mostly private)\n",
    "\n",
    "    ## this function constructs the Nytimes query, starting from some start date and end date and for some requested page\n",
    "    def __constructUrlNytimes(self, queryList, startDate='20160901', endDate='20160930', page=0):\n",
    "\n",
    "        apiUrl = 'http://api.nytimes.com/svc/search/v2/articlesearch.json?'\n",
    "\n",
    "        query = 'fq=news_desk:('\n",
    "        for iquery in queryList:\n",
    "            query = query + '\"' + iquery + '\"'\n",
    "        query += ')'\n",
    "        apiUrl += query\n",
    "\n",
    "        apiDate = 'begin_date=' + startDate + '&end_date=' + endDate + '&sort=newest' + \"&page=\" + str(page)\n",
    "\n",
    "        link = [apiUrl, apiDate, self.__offset, self.__nytApiKey]\n",
    "        reqUrl = '&'.join(link)\n",
    "        return reqUrl\n",
    "\n",
    "    ## function that constructs a feed\n",
    "    def __constructFeed(self, reqUrl):\n",
    "        try:\n",
    "            ### open the requested url\n",
    "            jstr = urllib.request.urlopen(reqUrl).read()\n",
    "\n",
    "            ### increase the number of used queries\n",
    "            self.__numberOfUsedQueries += 1\n",
    "\n",
    "            ### return the feed\n",
    "            return json.loads(jstr)\n",
    "        except HTTPError as e:\n",
    "            continue\n",
    "        \n",
    "\n",
    "    ## function that parse the feed and add it up to existing dictionary\n",
    "    def __feedToDicParser(self, dictionary, feed):\n",
    "        fake = False\n",
    "        feed = feed['response']['docs']\n",
    "\n",
    "        # this is the range the NYTimes gives you in one query\n",
    "        for i in range(0, len(feed)):\n",
    "            tempFeed = feed[i]\n",
    "            idf = tempFeed['_id']\n",
    "            dictionary[idf] = []\n",
    "\n",
    "            # for security remember the id also in the body\n",
    "            dictionary[idf].append(idf)\n",
    "            # add whether source is fake or not a fake\n",
    "            dictionary[idf].append(fake)\n",
    "            # add publication date\n",
    "            dictionary[idf].append(tempFeed['pub_date'])\n",
    "            # add web-url\n",
    "            dictionary[idf].append(tempFeed['web_url'])\n",
    "            # add in a snippet\n",
    "            dictionary[idf].append(tempFeed['snippet'])\n",
    "            # add in an abstract\n",
    "            dictionary[idf].append(tempFeed['abstract'])\n",
    "        return dictionary\n",
    "\n",
    "    ## function returns nyt snippets as a dictionary\n",
    "    def __getNYTsnippets(self, queryList, startDate='20160901', endDate='20160931', upperBoundOnPages=10):\n",
    "        dic = {}\n",
    "        totalNumberOfErrors = 10\n",
    "        numOfErrors = 0\n",
    "        if ((upperBoundOnPages > self.__totalNumberOfAllowedQueries)\n",
    "            or (self.__numberOfUsedQueries>self.__totalNumberOfAllowedQueries)):\n",
    "            print('NYT upper bound exceeded returning empty dictionary ')\n",
    "            return dic\n",
    "        else:\n",
    "            i = 0\n",
    "            while i in range(0, upperBoundOnPages):\n",
    "                try:\n",
    "                    ### get the url\n",
    "                    url = self.__constructUrlNytimes(queryList, startDate, endDate, page=i)\n",
    "                    print(url)\n",
    "                    ### get feed\n",
    "                    feed = self.__constructFeed(url)\n",
    "                    ### parse the feed and add it to dic\n",
    "                    dic = self.__feedToDicParser(dic, feed)\n",
    "                    print('DOING PAGE NR. :' + str(i))\n",
    "                    i += 1\n",
    "                    time.sleep(3)\n",
    "                except HTTPError as e:\n",
    "                    if numOfErrors <= totalNumberOfErrors:\n",
    "                        print('TOTAL NR. of ERRORS: ' + str(numOfErrors))\n",
    "                        numOfErrors += 1\n",
    "                        continue\n",
    "                    else:\n",
    "\n",
    "                        break\n",
    "            return dic\n",
    "\n",
    "    ## function that finds the url links in saved snippet dictionary\n",
    "    ### if time allows add checker whether you are reading the NYT snippet dictionary\n",
    "    def __findUrlLinks(self, nameOfDic):\n",
    "        urlList = []\n",
    "        dic = self.__safelyOpenDict(nameOfDic)\n",
    "        for k in dic.keys():\n",
    "            for idd in dic[k].keys():\n",
    "                urlList.append(dic[k][idd][3])\n",
    "        return urlList\n",
    "\n",
    "    ## in this prune away 'video' section and 'slideshow' section in saved snippet dic.\n",
    "    ## reason is too little text in there\n",
    "    ### if time allows add checker whether you are reading the NYT snippet dictionary\n",
    "    def __findUrlLinksPruned(self, nameOfDic):\n",
    "        urlList = []\n",
    "        dic = self.__safelyOpenDict(nameOfDic)\n",
    "        for k in dic.keys():\n",
    "            for idd in dic[k].keys():\n",
    "                tempLink = dic[k][idd][3]\n",
    "                if not (('video' in tempLink) or ('slideshow' in tempLink)):\n",
    "                    urlList.append(dic[k][idd][3])\n",
    "        return urlList\n",
    "\n",
    "    ## in this prune away 'video' section and 'slideshow' section in saved snippet dic.\n",
    "    ## reason is too little text in there\n",
    "    ### if time allows add checker whether you are reading the NYT snippet dictionary\n",
    "    def __findUrlLinksPrunedDictionary(self, nameOfDic):\n",
    "        urlListDict = {}\n",
    "        dic = self.__safelyOpenDict(nameOfDic)\n",
    "        for k in dic.keys():\n",
    "            for idd in dic[k].keys():\n",
    "                tempLink = dic[k][idd][3]\n",
    "                if not (('video' in tempLink) or ('slideshow' in tempLink)):\n",
    "                    urlListDict[idd] = dic[k][idd][3]\n",
    "        return urlListDict\n",
    "\n",
    "\n",
    "    ## function that forces system to wait random time, with upper bound on wait equals to maxTime\n",
    "    def __waitRandomTime(self, maxTime=7):\n",
    "        ## setting up the seed\n",
    "        tm = int(round(time.time()))\n",
    "        random.seed(tm)\n",
    "\n",
    "        tim = random.randint(0, 8 * 100) / 100.0\n",
    "        time.sleep(tim)\n",
    "\n",
    "    ## SAVING and ADD/DEL FUNCTIONS\n",
    "\n",
    "    ### adds up two dictionaries and updates the first one to conglomerate\n",
    "    def __addDict(self, final, temp):\n",
    "        z = dict(final, **temp)\n",
    "        final.update(z)\n",
    "\n",
    "\n",
    "    ### creates an empty dictionary\n",
    "    def __createEmptyDict(self, name_of_dictionary):\n",
    "        with open(name_of_dictionary + '.pickle', 'wb') as handle:\n",
    "            pickle.dump({}, handle)\n",
    "\n",
    "    ###  opens the dictionary from file, if file is not present it creates is\n",
    "    def __openDict(self, name_of_saved_dictionary):\n",
    "        try:\n",
    "            with open(name_of_saved_dictionary + '.pickle', 'rb') as handle:\n",
    "                return pickle.load(handle)\n",
    "        except FileNotFoundError:\n",
    "            print(\"File was not found, I will create empty dictionary with name \" + name_of_saved_dictionary)\n",
    "            self.__createEmptyDict(name_of_saved_dictionary)\n",
    "            return self.__openDict(name_of_saved_dictionary)\n",
    "\n",
    "    ### save list to file\n",
    "    def __saveList(self, listToSave, nameOfList):\n",
    "        if (nameOfList + '.pickle') not in os.listdir():\n",
    "            with open(nameOfList + '.pickle', 'wb') as handle:\n",
    "                pickle.dump(listToSave, handle)\n",
    "            print('List was saved under name ' + nameOfList + '.pickle')\n",
    "        else:\n",
    "            print('ERROR: the name ' + nameOfList + '.pickle has been used, use different one!')\n",
    "\n",
    "    ### saves dictionary to file\n",
    "    def __saveDictToFile(self, dictionary, name):\n",
    "        with open(name + '.pickle', 'wb') as handle:\n",
    "            pickle.dump(dictionary, handle)\n",
    "\n",
    "    ### saving function that saves the dictionary and if not such a dictionary is present on disc will create it\n",
    "    def __saveDict(self, dictionary, name_of_dictionary, start_of_the_month):\n",
    "\n",
    "        # I will try to open a dictionary\n",
    "        tempDic = self.__openDict(name_of_dictionary)\n",
    "\n",
    "        # add a dictionary to current dictionary if it was not there\n",
    "        if start_of_the_month not in tempDic.keys():\n",
    "            addThisDic = {}\n",
    "            addThisDic[start_of_the_month] = dictionary\n",
    "            self.__addDict(tempDic, addThisDic)\n",
    "\n",
    "            # saving dictionary into the file\n",
    "            self.__saveDictToFile(tempDic, name_of_dictionary)\n",
    "        else:\n",
    "            print('the month ' + start_of_the_month + ' has been found in dictionary ' + name_of_dictionary)\n",
    "\n",
    "    ### safely open the dictionary from file\n",
    "    def __safelyOpenDict(self, nameOfSavedDictionary):\n",
    "        try:\n",
    "            with open(nameOfSavedDictionary + '.pickle', 'rb') as handle:\n",
    "                return pickle.load(handle)\n",
    "        except FileNotFoundError:\n",
    "            print(\"File was not found \")\n",
    "\n",
    "    ### deleting a key from a dictionary\n",
    "    def __delPartKey(self, key, nameOfDic):\n",
    "        dic = self.__safelyOpenDict(nameOfDic)\n",
    "\n",
    "        if key in dic.keys():\n",
    "            del (dic[key])\n",
    "\n",
    "        self.__saveDictToFile(dic, nameOfDic)\n",
    "\n",
    "\n",
    "    ## SCRAPING functions\n",
    "\n",
    "    ## function that scrapes another link\n",
    "    ### add checkers whether types are matching\n",
    "    def __findAllInBs(self, beautifulSoup, textClass, itemprop=False, p='p'):\n",
    "        text = ''\n",
    "        if itemprop:\n",
    "            for txt in beautifulSoup.find_all(p, class_=textClass, itemprop=\"articleBody\"):\n",
    "                text = text + ' ' + txt.text\n",
    "            return text\n",
    "        else:\n",
    "            for txt in beautifulSoup.find_all(p, class_=textClass):\n",
    "                text = text + ' ' + txt.text\n",
    "            return text\n",
    "\n",
    "    ## function that extract text, for Nytimes we have several classes of the text\n",
    "    def __extractText(self, beautifulSoup, textClass=1):\n",
    "        text = ''\n",
    "        if textClass == 1:\n",
    "            cls = \"story-body-text\"\n",
    "            return self.__findAllInBs(beautifulSoup, cls, itemprop=True)\n",
    "        elif textClass == 2:\n",
    "            cls = \"story-body-text story-content\"\n",
    "            return self.__findAllInBs(beautifulSoup, cls)\n",
    "        elif textClass == 3:\n",
    "            ### seems to be valid for video section, that I pruned away\n",
    "            cls = \"content-description\"\n",
    "            return self.__findAllInBs(beautifulSoup, cls)\n",
    "\n",
    "    ## function that removes some garbage from the text\n",
    "    def __removeTextGarbage(self, text, textClass=1):\n",
    "        if textClass == 1:\n",
    "            text = re.sub('\\n', ' ', text)\n",
    "            text = text.split('.')\n",
    "            # get rid of two last sentences for text from textClass=1\n",
    "            del (text[-1])\n",
    "            del (text[-1])\n",
    "            # join the text back to pre-cleaned text\n",
    "            text = '. '.join(text)\n",
    "            text += '.'\n",
    "            return text\n",
    "\n",
    "    ## function that finds a missing part of dictionary B in dictionary A\n",
    "    def __findMissingDic(self, dicA, dicB):\n",
    "        tempDic = {}\n",
    "        for key in dicB.keys():\n",
    "            if key not in dicA.keys():\n",
    "                self.__addDict(tempDic, {key: dicB[key]})\n",
    "        return tempDic\n",
    "\n",
    "    ## this function adds and saves dictionaryToAdd to the saved dictionary called nameOfDictionary\n",
    "    def __saveDictNytFullText(self, dictionaryToAdd, nameOfDictionary):\n",
    "\n",
    "        # I will try to open a dictionary\n",
    "        tempDic = self.__openDict(nameOfDictionary)\n",
    "\n",
    "        # add a dictionary to current dictionary if it was not there\n",
    "        # find Missing compares the two dictionaries and returns ids of all members of dictionary that\n",
    "        # are not in tempDic\n",
    "        missingIntempDic = self.__findMissingDic(tempDic, dictionaryToAdd)\n",
    "        if len(missingIntempDic):\n",
    "            print('adding some missing stuff to dictionary')\n",
    "            self.__addDict(tempDic, missingIntempDic)\n",
    "\n",
    "            # saving dictionary into the file\n",
    "            self.__saveDictToFile(tempDic, nameOfDictionary)\n",
    "        else:\n",
    "            print('Everything seems to be already in the saved dictionary ' + nameOfDictionary + '.pickle')\n",
    "\n",
    "\n",
    "    ## function retrieves the text and if necessary adds to a badLinkList, NOTICE NOTICE NOTICE this function depends\n",
    "    ## on number of text classes you have\n",
    "    def __getText(self, link, badLinkList, numberOfTextClasses):\n",
    "        cj = CookieJar()\n",
    "        # this part is responsible for opening and retrieving the raw data from NYTimes\n",
    "        opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj))\n",
    "        response = opener.open(link)\n",
    "        rd = response.read()\n",
    "        bss = bs(rd, \"html.parser\")\n",
    "\n",
    "        # this part is responsible for getting the nonzero text\n",
    "        textClass = 1\n",
    "        text = ''\n",
    "        while textClass <= numberOfTextClasses:\n",
    "            print(textClass)\n",
    "            tempText = self.__extractText(bss, textClass=textClass)\n",
    "            if len(tempText) > len(text):\n",
    "                text = tempText\n",
    "            textClass += 1\n",
    "        # if number of characters is smaller than 55 chars something is wrong, put link into the badLinkList\n",
    "        if len(text) < 55:\n",
    "            badLinkList.append(text)\n",
    "            print('Check the bad link list, because there are ' + str(len(badLinkList)) + ' already ' +\n",
    "                  'and one was just added! It is possible that there is another textClass among them or something else'\n",
    "                  ' is not working properly.')\n",
    "        return text\n",
    "\n",
    "    ## this function queries the NYT server, scrape the page and retrieve text, save it to: nameOfFile\n",
    "    ## remember use the pruned dictionary\n",
    "    def __downloadNytText(self, urlDic, nameOfFile):\n",
    "        if (nameOfFile + '.pickle') in os.listdir():\n",
    "            print(\n",
    "                'ERROR: ALREADY USED name for requested file: ' + nameOfFile + '.pickle' + ',   USE different filename!')\n",
    "        else:\n",
    "            totalNumberOfErrors = 1000\n",
    "            numberOfErrors = 0\n",
    "            finalDic = {}\n",
    "            badLinkList = []\n",
    "            counter = 1\n",
    "\n",
    "            ### we have to catch the HTTP errors\n",
    "            for articleKey in urlDic.keys():\n",
    "                try:\n",
    "                    print('work in progress, step: ' + str(counter))\n",
    "                    print('working on article with id: ' + articleKey)\n",
    "\n",
    "                    # print(urlDic[articleKey])\n",
    "                    text = self.__getText(urlDic[articleKey], badLinkList, 2)\n",
    "                    # print(text)\n",
    "                    counter += 1\n",
    "\n",
    "                    if len(text) < 55:\n",
    "                        print('continue')\n",
    "                        continue\n",
    "                    else:\n",
    "                        finalDic[articleKey] = text\n",
    "\n",
    "                    ## for sureness save it to disc immediately every 20 articles:\n",
    "                    if (counter % 20) == 0:\n",
    "                        self.__saveDictNytFullText(finalDic, nameOfFile)\n",
    "\n",
    "                    ## waiting some random time to not be blocked (at least fast)\n",
    "                    self.__waitRandomTime()\n",
    "\n",
    "                except HTTPError as e:\n",
    "                    numberOfErrors += 1\n",
    "                    print('ERROR nr. ' + str(numberOfErrors))\n",
    "                    print('there was some HTTP error, it might be they have blocked you, '\n",
    "                          'I will wait for some longer time and will query again')\n",
    "                    print('I will do that until total number errors is smaller than ' + str(totalNumberOfErrors))\n",
    "\n",
    "                    if numberOfErrors <= totalNumberOfErrors:\n",
    "                        self.__waitRandomTime(15)\n",
    "                        continue\n",
    "                    else:\n",
    "                        print('-----------END------BECAUSE---HTTP----ERROR---DID---NOT--STOP')\n",
    "                        print('last article id was: ' + str(articleKey))\n",
    "                except urllib.error.URLError as e:\n",
    "                    print('URLError---URLError---URLError--URLError--URLError')\n",
    "                    numberOfErrors += 1\n",
    "                    print('ERROR nr. ' + str(numberOfErrors))\n",
    "                    print('there was some HTTP error, it might be they have blocked you, '\n",
    "                          'I will wait for some longer time and will query again')\n",
    "                    print('I will do that until total number errors is smaller than ' + str(totalNumberOfErrors))\n",
    "\n",
    "                    if numberOfErrors <= totalNumberOfErrors:\n",
    "                        self.__waitRandomTime(15)\n",
    "                        continue\n",
    "                    else:\n",
    "                        print('-----------END------BECAUSE---HTTP----ERROR---DID---NOT--STOP')\n",
    "                        print('last article id was: ' + str(articleKey))\n",
    "\n",
    "            ## saving what I have in final dic\n",
    "            print('saving what I have in final dic ')\n",
    "            self.__saveDictNytFullText(finalDic, nameOfFile)\n",
    "\n",
    "            return badLinkList\n",
    "\n",
    "    ## by this function you are getting the text from Nytimes, the year you are getting is dependent ONLY on\n",
    "    ## the name of the dictionary nameOfUrlLinksDictionary\n",
    "    def __getNytText(self, nameOfTextDictOnDisk, nameOfUrlLinksDictionary, year):\n",
    "\n",
    "        ### creating the customary names\n",
    "        nameOfTextDictOnDisk = nameOfTextDictOnDisk + str(year)\n",
    "        nameOfUrlLinksDictionary = nameOfUrlLinksDictionary + str(year)\n",
    "\n",
    "\n",
    "        urlDic = self.__findUrlLinksPrunedDictionary(nameOfUrlLinksDictionary)\n",
    "        print('creating the url dictionary that is pruned')\n",
    "\n",
    "        if len(urlDic) > 0:\n",
    "            print('downloading the Nyt for the Url dictionary ' + nameOfUrlLinksDictionary)\n",
    "            badLinkList = self.__downloadNytText(urlDic, nameOfTextDictOnDisk)\n",
    "            if len(badLinkList) > 0:\n",
    "                print('there are some links that have not been turned into text see saved badLinkList for current year')\n",
    "                self.__saveList(badLinkList, ('badLinkList' + str(year)))\n",
    "            else:\n",
    "                print('everything seems to be turned into text check results saved in: ' + nameOfTextDictOnDisk)\n",
    "        else:\n",
    "            print('CHECK: for some reason the created urlDic is empty')\n",
    "\n",
    "    ## querying through months\n",
    "    def __queryThroughMonths(self, queryList, year=2016):\n",
    "        for month in range(1, 13):\n",
    "            try:\n",
    "                print('dealing with month nr.: ' + str(month))\n",
    "                rang = calendar.monthrange(year, month)\n",
    "                start = datetime.date(year=year, month=month, day=1).strftime(\"%Y%m%d\")\n",
    "                end = datetime.date(year=year, month=month, day=rang[1]).strftime(\"%Y%m%d\")\n",
    "\n",
    "                docName = 'nytSnippets' + str(year)\n",
    "                if (docName + '.pickle') in os.listdir():\n",
    "                    temp = self.__safelyOpenDict(docName)\n",
    "\n",
    "                    if start not in temp.keys():\n",
    "                        dic = self.__getNYTsnippets(queryList, startDate=start, endDate=end)\n",
    "                        self.__saveDict(dic, docName, start)\n",
    "                    else:\n",
    "                        print('skipping month ' + start + ' because already in database')\n",
    "                else:\n",
    "                    dic = self.__getNYTsnippets(queryList, startDate=start, endDate=end)\n",
    "                    self.__saveDict(dic, docName, start)\n",
    "\n",
    "                print('finished and saved month nr.: ' + str(month) + 'for year :' + str(year))\n",
    "\n",
    "            except HTTPError as e:\n",
    "                logging.error(\"HTTPError: with code: %s and with reason: %s\", e.code, e.reason)\n",
    "                print('there was some error in month ' + str(month) + ' see the logs')\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "    ### !!!!!!!!!!------->>>>>> NOTE NOTE NOTE not tested function\n",
    "\n",
    "    ## final function that allows you to get data between years startYear and endYear\n",
    "    def getDataNytimes(self, startYear, endYear):\n",
    "        ### looping through the years\n",
    "        for year in range(startYear, endYear + 1):\n",
    "\n",
    "            ### if number of queries is bigger than total number of queries allowed bail\n",
    "            if self.__numberOfUsedQueries> self.__totalNumberOfAllowedQueries:\n",
    "                break\n",
    "            else:\n",
    "                print('START querying the year: ' + str(year))\n",
    "                print('TOTAL number of queries used: ' + str(self.__numberOfUsedQueries))\n",
    "\n",
    "                ### this queries through months and saves result\n",
    "                self.__queryThroughMonths(self.__queryList, year)\n",
    "\n",
    "                nameOfSnippetDictionary = 'nytSnippets' + str(year)\n",
    "                print('FINISHED querying for the year: ' + year + 'checking whether you have file: '\n",
    "                      + nameOfSnippetDictionary + '.pickle saved in directory')\n",
    "\n",
    "                if (nameOfSnippetDictionary + '.pickle') in os.listdir():\n",
    "                    print(nameOfSnippetDictionary + '.pickle is in directory')\n",
    "                    print('everything seems to be OK')\n",
    "                else:\n",
    "                    print('ERROR:' + nameOfSnippetDictionary + '.pickle was NOT found directory')\n",
    "                    break\n",
    "\n",
    "\n",
    "                ### in this part you creates the pruned dictionary to be used in actual downloading of the data\n",
    "                ### this is for current year\n",
    "                dicToDownload = self.__findUrlLinksPrunedDictionary(nameOfSnippetDictionary)\n",
    "\n",
    "                ### downloading the dicToDownload\n",
    "                print('starting to download the text for year: ' + str(year))\n",
    "\n",
    "                print('first check whether you already do not have the file on disc if yes skip and report')\n",
    "                nameOfRawTextDictionary = 'nytRawText'+str(year)+'.pickle'\n",
    "\n",
    "                if nameOfRawTextDictionary not in os.listdir():\n",
    "                    print('the raw text dictionary with name: ' + nameOfRawTextDictionary + '.pickle' +\n",
    "                          ' has not been found on local drive. I will proceed to download it.')\n",
    "                    badLinkList = self.__getNytText('nytRawText', 'nytSnippets', year)\n",
    "                    print('stopped the text download for year ' + str(year))\n",
    "                else:\n",
    "                    print('the raw text dictionary with name: ' + nameOfRawTextDictionary + '.pickle' +\n",
    "                          ' HAS BEEN found on local drive. I will NOT download it.')\n",
    "                    print('please check whether you have everything you wanted in ' +\n",
    "                          nameOfRawTextDictionary + '.pickle')\n",
    "\n",
    "                ### checking whether you have saved at least something on disc\n",
    "                if nameOfRawTextDictionary not in os.listdir():\n",
    "                    print('the raw text dictionary with name: ' + nameOfRawTextDictionary + '.pickle' +\n",
    "                              ' has not been found on local drive. I will proceed to download it.')\n",
    "\n",
    "                else:\n",
    "                    print('the raw text dictionary with name: ' + nameOfRawTextDictionary + '.pickle' +\n",
    "                              ' HAS BEEN found on local drive. SOMETHING is wrong!!!!')\n",
    "                    break\n",
    "\n",
    "\n",
    "                        ### waiting some random time in order not to be blocked or at least not that fast\n",
    "        return badLinkList\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## try tu query using NytData\n",
    "apiKey = '923f498f9f764c228a68501cf957355b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataBigger = NytData(apiKey, 'Nytimes', 7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START querying the year: 2015\n",
      "TOTAL number of queries used: 0\n",
      "dealing with month nr.: 1\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?fq=news_desk:(\"Politics\"\"Economics\"\"Business\"\"Financial\"\"World\"\"Washington\"\"Wealth\"\"Jobs\"\"Society\")&begin_date=20150101&end_date=20150131&sort=newest&page=0&offset=0&api-key=923f498f9f764c228a68501cf957355b\n",
      "DOING PAGE NR. :0\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?fq=news_desk:(\"Politics\"\"Economics\"\"Business\"\"Financial\"\"World\"\"Washington\"\"Wealth\"\"Jobs\"\"Society\")&begin_date=20150101&end_date=20150131&sort=newest&page=1&offset=0&api-key=923f498f9f764c228a68501cf957355b\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-f25b06144676>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataBigger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetDataNytimes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2015\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2016\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-cb5a3d37d304>\u001b[0m in \u001b[0;36mgetDataNytimes\u001b[0;34m(self, startYear, endYear)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m                 \u001b[0;31m### this queries through months and saves result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__queryThroughMonths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__queryList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 \u001b[0mnameOfSnippetDictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'nytSnippets'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-cb5a3d37d304>\u001b[0m in \u001b[0;36m__queryThroughMonths\u001b[0;34m(self, queryList, year)\u001b[0m\n\u001b[1;32m    473\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipping month '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' because already in database'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                     \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getNYTsnippets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartDate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendDate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__saveDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-cb5a3d37d304>\u001b[0m in \u001b[0;36m__getNYTsnippets\u001b[0;34m(self, queryList, startDate, endDate, upperBoundOnPages)\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0;31m### get feed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                     \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__constructFeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m                     \u001b[0;31m### parse the feed and add it to dic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__feedToDicParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-cb5a3d37d304>\u001b[0m in \u001b[0;36m__constructFeed\u001b[0;34m(self, reqUrl)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m### open the requested url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mjstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreqUrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;31m### increase the number of used queries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pipjak/anaconda/envs/py36/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pipjak/anaconda/envs/py36/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pipjak/anaconda/envs/py36/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 642\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pipjak/anaconda/envs/py36/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pipjak/anaconda/envs/py36/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pipjak/anaconda/envs/py36/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "dataBigger.getDataNytimes(2015,2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
