{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bigger data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import text_to_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = ['trueNytMikro']\n",
    "fake = ['fakeKaggleMikro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n1\n"
     ]
    }
   ],
   "source": [
    "small_model = text_to_model.text_to_model(true, fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving data: goldenTrue, goldenFake, dataTrue, dataFake\nstarting the NLP calculations\nThe dictionary you are putting in should be already in form: {text_id: text}.\nNothing to do on not fake news, the text seemed clean already.\nNOTE: you are creating the fake = False class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\nThe dictionary you are putting in should be already in form: {text_id: text}.\nNOTE: you are creating the fake = True class.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\nstarting the nlp of true set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 1\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 2\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 3\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 4\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 5\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 6\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 7\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 8\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 9\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 10\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 11\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 13\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 14\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 15\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 16\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 17\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 18\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 19\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 20\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 21\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 22\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 23\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 24\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 25\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 26\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 27\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\nthe NLPanalysis class saves resulting dictionary as nlp_true_data_2017_2_4_18_56_30Falsepickle\nnlp of true set finished\nstarting nlp of false set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 1\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 2\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 3\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 4\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 5\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 6\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 7\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 8\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 9\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 10\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 11\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 12\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 13\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 14\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 15\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 16\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 17\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 18\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 19\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 20\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 21\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 22\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 23\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 24\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 25\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 26\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 27\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 28\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving, number of dictionary key is: 29\nNOTICE\nthe same named file found on disc, be super that you are not rewriting it in good way\nthe NLPanalysis class saves resulting dictionary as nlp_true_data_2017_2_4_18_56_30Falsepickle\nstarting the non - NLP calculations\n[5, 0.026737967914438502, 9.76, 0.07651515151515151, 0.33560606060606063, 0, 0.0, 0, 0.0, 0.016371301167433894, 0.08313129736994858, 59.26949152542373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference is not empty, is of size: 0\nwe add it to self.__meta, that is of size: 71\nwe added missing urls metadata to self.__meta, now it is of size: 71\nsaving the meta on disc\n[5, 0.026737967914438502, 9.76, 0.07651515151515151, 0.33560606060606063, 0, 0.0, 0, 0.0, 0.016371301167433894, 0.08313129736994858, 59.26949152542373, 0.14185106490048782]\n............NOTICE NOTICE\ncontrol length before was 12\ncontrol length after was 13\n............NOTICE NOTICE\ndifference is not empty, is of size: 0\nwe add it to self.__meta, that is of size: 220\nwe added missing urls metadata to self.__meta, now it is of size: 220\nsaving the meta on disc\nstarting the model-building part\nbuilding AdaBoost classifier with grid search\nreturning: @ 0 = training set, @ 1 = testing set, @ 2 = validation set\nnumber of trainX features is: (40, 13)\nprint first row in trainX \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      31.000000\n1       0.048589\n2      10.730000\n3       0.037963\n4       0.454507\n5       1.000000\n6       0.001567\n7       1.000000\n8       0.001567\n9       0.126241\n10      0.045290\n11    130.046006\n12      0.937027\nName: aff3ede5c01b6823e3bb6f96035c4df02d470d98, dtype: float64\nshape of trainY is: (40,)\nstart grid-search for ada - boost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pipjak/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/utils/class_weight.py:65: DeprecationWarning: The class_weight='auto' heuristic is deprecated in 0.17 in favor of a new heuristic class_weight='balanced'. 'auto' will be removed in 0.19\n  \" 0.19\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pipjak/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/utils/class_weight.py:65: DeprecationWarning: The class_weight='auto' heuristic is deprecated in 0.17 in favor of a new heuristic class_weight='balanced'. 'auto' will be removed in 0.19\n  \" 0.19\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pipjak/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/utils/class_weight.py:65: DeprecationWarning: The class_weight='auto' heuristic is deprecated in 0.17 in favor of a new heuristic class_weight='balanced'. 'auto' will be removed in 0.19\n  \" 0.19\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pipjak/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/utils/class_weight.py:65: DeprecationWarning: The class_weight='auto' heuristic is deprecated in 0.17 in favor of a new heuristic class_weight='balanced'. 'auto' will be removed in 0.19\n  \" 0.19\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pipjak/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/utils/class_weight.py:65: DeprecationWarning: The class_weight='auto' heuristic is deprecated in 0.17 in favor of a new heuristic class_weight='balanced'. 'auto' will be removed in 0.19\n  \" 0.19\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pipjak/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/utils/class_weight.py:65: DeprecationWarning: The class_weight='auto' heuristic is deprecated in 0.17 in favor of a new heuristic class_weight='balanced'. 'auto' will be removed in 0.19\n  \" 0.19\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pipjak/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/utils/class_weight.py:65: DeprecationWarning: The class_weight='auto' heuristic is deprecated in 0.17 in favor of a new heuristic class_weight='balanced'. 'auto' will be removed in 0.19\n  \" 0.19\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pipjak/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/utils/class_weight.py:65: DeprecationWarning: The class_weight='auto' heuristic is deprecated in 0.17 in favor of a new heuristic class_weight='balanced'. 'auto' will be removed in 0.19\n  \" 0.19\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pipjak/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/utils/class_weight.py:65: DeprecationWarning: The class_weight='auto' heuristic is deprecated in 0.17 in favor of a new heuristic class_weight='balanced'. 'auto' will be removed in 0.19\n  \" 0.19\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pipjak/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/utils/class_weight.py:65: DeprecationWarning: The class_weight='auto' heuristic is deprecated in 0.17 in favor of a new heuristic class_weight='balanced'. 'auto' will be removed in 0.19\n  \" 0.19\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pipjak/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/utils/class_weight.py:65: DeprecationWarning: The class_weight='auto' heuristic is deprecated in 0.17 in favor of a new heuristic class_weight='balanced'. 'auto' will be removed in 0.19\n  \" 0.19\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pipjak/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/utils/class_weight.py:65: DeprecationWarning: The class_weight='auto' heuristic is deprecated in 0.17 in favor of a new heuristic class_weight='balanced'. 'auto' will be removed in 0.19\n  \" 0.19\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the model as: AdaBoost2017_2_4_18_58_20.pkl\nTest set accuracy score for the model AdaBoost2017_2_4_18_58_20 is 1.0\nValidation set accuracy score for the model AdaBoost2017_2_4_18_58_20 is 1.0\nada - boost grid search finished it is saved on local drive as (note without .pickle): AdaBoost2017_2_4_18_58_20\nmodel is also stored in local instance of Train class, use method get_model() to retrieve it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pipjak/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/utils/class_weight.py:65: DeprecationWarning: The class_weight='auto' heuristic is deprecated in 0.17 in favor of a new heuristic class_weight='balanced'. 'auto' will be removed in 0.19\n  \" 0.19\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "small_model.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n '.git',\n '.idea',\n '.ipynb_checkpoints',\n '__pycache__',\n 'AdaBoost2017_2_1_10_34_32',\n 'AdaBoost2017_2_1_12_32_45',\n 'AdaBoost2017_2_1_12_46_35',\n 'AdaBoost2017_2_1_15_37_11',\n 'AdaBoost2017_2_1_15_49_1',\n 'AdaBoost2017_2_4_18_32_36',\n 'AdaBoost2017_2_4_18_58_20',\n 'AdaBoost_ADA_MODEL',\n 'automated_analysis.ipynb',\n 'badLinkList2013.pickle',\n 'badLinkList2014.pickle',\n 'badLinkList2015.pickle',\n 'bigger_data_set_model.ipynb',\n 'custom_profanity.py',\n 'dataFake2017_2_1_10_1_57.pickle',\n 'dataFake2017_2_1_10_5_25.pickle',\n 'dataFake2017_2_1_12_13_41.pickle',\n 'dataFake2017_2_1_12_18_37.pickle',\n 'dataFake2017_2_1_12_31_17.pickle',\n 'dataFake2017_2_1_12_44_59.pickle',\n 'dataFake2017_2_1_12_8_44.pickle',\n 'dataFake2017_2_1_15_35_32.pickle',\n 'dataFake2017_2_1_15_43_46.pickle',\n 'dataFake2017_2_1_15_47_21.pickle',\n 'dataFake2017_2_1_9_21_13.pickle',\n 'dataFake2017_2_1_9_24_4.pickle',\n 'dataFake2017_2_1_9_49_23.pickle',\n 'dataFake2017_2_4_18_30_20.pickle',\n 'dataFake2017_2_4_18_35_25.pickle',\n 'dataFake2017_2_4_18_56_30.pickle',\n 'dataFake2017_2_4_1_3_21.pickle',\n 'dataTrue2017_2_1_10_1_57.pickle',\n 'dataTrue2017_2_1_10_5_25.pickle',\n 'dataTrue2017_2_1_12_13_41.pickle',\n 'dataTrue2017_2_1_12_18_37.pickle',\n 'dataTrue2017_2_1_12_31_17.pickle',\n 'dataTrue2017_2_1_12_44_59.pickle',\n 'dataTrue2017_2_1_12_8_44.pickle',\n 'dataTrue2017_2_1_15_35_32.pickle',\n 'dataTrue2017_2_1_15_43_46.pickle',\n 'dataTrue2017_2_1_15_47_21.pickle',\n 'dataTrue2017_2_1_9_21_13.pickle',\n 'dataTrue2017_2_1_9_24_4.pickle',\n 'dataTrue2017_2_1_9_49_23.pickle',\n 'dataTrue2017_2_4_18_30_20.pickle',\n 'dataTrue2017_2_4_18_35_24.pickle',\n 'dataTrue2017_2_4_18_56_30.pickle',\n 'dataTrue2017_2_4_1_3_20.pickle',\n 'dict_words.txt',\n 'dirty_data_code_NYT.py',\n 'fake.csv',\n 'fake_data.py',\n 'fake_urls_metadata.pickle',\n 'FakeData.ipynb',\n 'fakeDictionaryTrialNUMBERSTrue.pickle',\n 'fakeKaggleEnglishOver300.pickle',\n 'fakeKaggleMikro.pickle',\n 'fakeKaggleSmall.pickle',\n 'getting_data.ipynb',\n 'goldenFake.pickle',\n 'goldenFake2017_2_1_10_1_57.pickle',\n 'goldenFake2017_2_1_10_5_25.pickle',\n 'goldenFake2017_2_1_12_13_41.pickle',\n 'goldenFake2017_2_1_12_18_37.pickle',\n 'goldenFake2017_2_1_12_31_17.pickle',\n 'goldenFake2017_2_1_12_44_59.pickle',\n 'goldenFake2017_2_1_12_8_44.pickle',\n 'goldenFake2017_2_1_15_35_32.pickle',\n 'goldenFake2017_2_1_15_43_46.pickle',\n 'goldenFake2017_2_1_15_47_21.pickle',\n 'goldenFake2017_2_1_9_21_12.pickle',\n 'goldenFake2017_2_1_9_24_3.pickle',\n 'goldenFake2017_2_1_9_49_22.pickle',\n 'goldenFake2017_2_4_18_30_20.pickle',\n 'goldenFake2017_2_4_18_35_24.pickle',\n 'goldenFake2017_2_4_18_56_30.pickle',\n 'goldenFake2017_2_4_1_3_19.pickle',\n 'goldenFakeVector.pickle',\n 'goldenTrue.pickle',\n 'goldenTrue2017_2_1_10_1_57.pickle',\n 'goldenTrue2017_2_1_10_5_25.pickle',\n 'goldenTrue2017_2_1_12_13_41.pickle',\n 'goldenTrue2017_2_1_12_18_37.pickle',\n 'goldenTrue2017_2_1_12_31_17.pickle',\n 'goldenTrue2017_2_1_12_44_59.pickle',\n 'goldenTrue2017_2_1_12_8_44.pickle',\n 'goldenTrue2017_2_1_15_35_32.pickle',\n 'goldenTrue2017_2_1_15_43_46.pickle',\n 'goldenTrue2017_2_1_15_47_21.pickle',\n 'goldenTrue2017_2_1_9_21_13.pickle',\n 'goldenTrue2017_2_1_9_24_4.pickle',\n 'goldenTrue2017_2_1_9_49_22.pickle',\n 'goldenTrue2017_2_4_18_30_20.pickle',\n 'goldenTrue2017_2_4_18_35_24.pickle',\n 'goldenTrue2017_2_4_18_56_30.pickle',\n 'goldenTrue2017_2_4_1_3_20.pickle',\n 'goldenTrueVector.pickle',\n 'helpful_functions.py',\n 'kucapaca.pickle',\n 'LargerNytDataSetQuery.ipynb',\n 'micro_batch.ipynb',\n 'model_building.ipynb',\n 'model_small_batch_automatic.ipynb',\n 'newDict.pickle',\n 'nlp_analysis.py',\n 'nlp_fake_data_2017_2_1_10_5_54True.pickle',\n 'nlp_fake_data_2017_2_1_12_13_43True.pickle',\n 'nlp_fake_data_2017_2_1_12_18_38True.pickle',\n 'nlp_fake_data_2017_2_1_12_31_19True.pickle',\n 'nlp_fake_data_2017_2_1_12_45_1True.pickle',\n 'nlp_fake_data_2017_2_1_12_8_46True.pickle',\n 'nlp_fake_data_2017_2_1_15_35_34True.pickle',\n 'nlp_fake_data_2017_2_1_15_43_48True.pickle',\n 'nlp_fake_data_2017_2_1_15_47_23True.pickle',\n 'nlp_fake_data_2017_2_4_18_30_23True.pickle',\n 'nlp_fake_data_2017_2_4_18_56_32True.pickle',\n 'nlp_true_data_2017_2_1_10_5_25False.pickle',\n 'nlp_true_data_2017_2_1_12_13_41False.pickle',\n 'nlp_true_data_2017_2_1_12_18_37False.pickle',\n 'nlp_true_data_2017_2_1_12_31_17False.pickle',\n 'nlp_true_data_2017_2_1_12_44_59False.pickle',\n 'nlp_true_data_2017_2_1_12_8_44False.pickle',\n 'nlp_true_data_2017_2_1_15_35_32False.pickle',\n 'nlp_true_data_2017_2_1_15_43_46False.pickle',\n 'nlp_true_data_2017_2_1_15_47_21False.pickle',\n 'nlp_true_data_2017_2_1_9_49_23False.pickle',\n 'nlp_true_data_2017_2_4_18_30_20False.pickle',\n 'nlp_true_data_2017_2_4_18_56_30False.pickle',\n 'NLPanalysis.ipynb',\n 'numericalFakeFromKaggle.pickle',\n 'numericalTrue2016NYT.pickle',\n 'nyt.csv',\n 'nyt.jsongetTimesArticles_testing.log',\n 'nyt_data.py',\n 'nytRawTestText2016.pickle',\n 'nytRawText2012.pickle',\n 'nytRawText2013.pickle',\n 'nytRawText2014.pickle',\n 'nytRawText2015.pickle',\n 'nytRawText2016.pickle',\n 'nytSnippets2012.pickle',\n 'nytSnippets2013.pickle',\n 'nytSnippets2014.pickle',\n 'nytSnippets2015.pickle',\n 'nytSnippets2016.pickle',\n 'outside_nlp_features.ipynb',\n 'Predict.py',\n 'profane_words.txt',\n 'profanity',\n 'profanity_checker.ipynb',\n 'profanity_xx',\n 'PutDataIntoSQL.ipynb',\n 'PutDataIntoSQL.py',\n 'python_whois',\n 'testDic.pickle',\n 'testing_various_functions.ipynb',\n 'text_to_model.py',\n 'train.py',\n 'TrainAndPredict.ipynb',\n 'trial.pickle',\n 'trialFakeNUMBERSTrue.pickle',\n 'trialList.pickle',\n 'trialList1.pickle',\n 'true_urls_metadata.pickle',\n 'trueDictionaryTrialNUMBERSFalse.pickle',\n 'trueNytMikro.pickle',\n 'twoTrialFakeNUMBERSTrue.pickle',\n 'url_analysis.py',\n 'utilities.py']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### now lets do the big set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = ['nytRawText2016', 'nytRawText2015', 'nytRawText2014', 'nytRawText2013', \n",
    "        'nytRawText2012']\n",
    "fake = ['fakeKaggleEnglishOver300']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n1\n"
     ]
    }
   ],
   "source": [
    "fin_model = text_to_model.text_to_model(true, fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving data: goldenTrue, goldenFake, dataTrue, dataFake\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting the NLP calculations\nThe dictionary you are putting in should be already in form: {text_id: text}.\nNothing to do on not fake news, the text seemed clean already.\nNOTE: you are creating the fake = False class.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The `text` argument passed to `__init__(text)` must be a string, not <class 'dict'>",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-3619611aa31f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfin_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Pipjak/Desktop/WORK/Programovanie_s_Lenuskou/INSIGHT_DATA/FAKE_NEWS_CHECKER/text_to_model.py\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(self, name_of_model)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dataTrue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dataTrue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_true_keys\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0;31m# find golden true and golden fake vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;31m# golden vectors are defined to be 20 percent of each dataFake and dataTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mlen_golden_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dataFake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pipjak/Desktop/WORK/Programovanie_s_Lenuskou/INSIGHT_DATA/FAKE_NEWS_CHECKER/nlp_analysis.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, golden_fake, golden_true, dictionary, name_of_final_dictionary_on_disc, fake)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;31m# pre-compute the golden vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__precompute_golden_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pipjak/Desktop/WORK/Programovanie_s_Lenuskou/INSIGHT_DATA/FAKE_NEWS_CHECKER/nlp_analysis.py\u001b[0m in \u001b[0;36m__precompute_golden_vectors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m                                          self.__golden_fake.keys()]\n\u001b[1;32m     82\u001b[0m             self.__golden_true_vector = [self.__text_to_vector(self.__golden_true[key]) for key in\n\u001b[0;32m---> 83\u001b[0;31m                                          self.__golden_true.keys()]\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__save_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__golden_fake_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'goldenFakeVector'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pipjak/Desktop/WORK/Programovanie_s_Lenuskou/INSIGHT_DATA/FAKE_NEWS_CHECKER/nlp_analysis.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     80\u001b[0m             self.__golden_fake_vector = [self.__text_to_vector(self.__golden_fake[key]) for key in\n\u001b[1;32m     81\u001b[0m                                          self.__golden_fake.keys()]\n\u001b[0;32m---> 82\u001b[0;31m             self.__golden_true_vector = [self.__text_to_vector(self.__golden_true[key]) for key in\n\u001b[0m\u001b[1;32m     83\u001b[0m                                          self.__golden_true.keys()]\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pipjak/Desktop/WORK/Programovanie_s_Lenuskou/INSIGHT_DATA/FAKE_NEWS_CHECKER/nlp_analysis.py\u001b[0m in \u001b[0;36m__text_to_vector\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;31m# this function turns the text into a vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__text_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__relevant_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pipjak/Desktop/WORK/Programovanie_s_Lenuskou/INSIGHT_DATA/FAKE_NEWS_CHECKER/nlp_analysis.py\u001b[0m in \u001b[0;36m__relevant_words\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;31m# this function picks up the relevant words from the text == words of type 'NN' and 'JJ' in nltk language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__relevant_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"NN\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"JJ\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pipjak/anaconda/envs/py36/lib/python3.6/site-packages/textblob/blob.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, text, tokenizer, pos_tagger, np_extractor, analyzer, parser, classifier, clean_html)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             raise TypeError('The `text` argument passed to `__init__(text)` '\n\u001b[0;32m--> 344\u001b[0;31m                             'must be a string, not {0}'.format(type(text)))\n\u001b[0m\u001b[1;32m    345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclean_html\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             raise NotImplementedError(\"clean_html has been deprecated. \"\n",
      "\u001b[0;31mTypeError\u001b[0m: The `text` argument passed to `__init__(text)` must be a string, not <class 'dict'>"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "fin_model.create_model(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}